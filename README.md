# Chicaho_crime_BIG-DATA-Analysis
We will be Analyzing the Chicago Crime Data and shedding light on some useful insights while analyzing the data. We treat this data a Big Data as the crime reports from 2021 to present as of November 2021 so the number of observations is about 7,428,804. Chicago crime data set from 2021 to the present, Using Big Query API, the data is extracted from the City of Chicago public website: which would be in JSON/CSV , the analysis is done on virtual machine on Jetstream on the Pyspark-Ubuntu18_04-instance, using the Jupyter notebook a schema is provided for the dataset, analysis the data using the Date Time module in PySpark o the Spark data frame using the RDD functions, insights and statistics from the data will be provided, trend over the years and use pyspark.ml.feature to build a model and predict the what kind of crime will be committed from the given data variables and then push the files to GitHub. The pipeline will resemble several topics and technologies topics learnt in INFO-I535.
PySpark is an excellent language for doing large-scale exploratory data analysis, machine learning pipelines, and data platform ETLs. PySpark is an excellent language to use if we are already familiar with Python and libraries like Pandas. It'll help you construct more scalable analytics and pipelines
Big Query is ideal for swiftly storing and querying massive data sets. Google Cloud SQL is mostly built on RDBMS (Relational Database Management System) ideas. It supports MySQL and PostgreSQL databases. Although Big Query is best suited for analytics, it can also handle transactional data. Because Big Query is a Datawarehouse with the potential to query ridiculously enormous data sets and produce responses instantaneously, it is far quicker than Querying in Cloud SQL. A Big Query API is used to build client libraries, IDE plugins, and other tools that interact with Google APIs. Here, Apache Spark is used to read, manipulate, and query data. We Make use of current Python packages to visualize the data. Matplotlib is used at for visualization, also use of Seaborn where Spark and Matplotlib interoperability is a problem, utilization of Pandas and NumPy is done. Spark is a big data processing framework that processes data 100 times faster than Hadoop in memory. Virtualization allows users to separate operating systems from the underlying hardware, allowing them to run different operating systems simultaneously on a single physical machine, such as Windows and Linux. Guests OSs are the name for such operating systems (operating systems). Virtualization is a technique that uses software to create an abstraction layer over computer hardware, allowing physical components such as processors, memory, and storage to be split into numerous virtual pieces (also known as virtual machines). Based on Atmosphere and OpenStack, Jetstream is a user-friendly cloud computing platform for researchers. It is intended to provide adjustable cyberinfrastructure that allows you to have on-demand access to interactive computing and data analysis capabilities, anytime and wherever you need to examine your data.
